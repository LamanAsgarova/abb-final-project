Introduction to Statistical Learning
Laman Asgarova
Laman.Asgarova@abb-bank.az
August 15, 2025
1 Ch. 4: Classification
1.1 Conceptual:
Task 1:
Starting from the logistic function form:
eβ0+β1X
p(X) = . (4.2)
1+eβ0+β1X
First, compute 1−p(X):
eη 1+eη −eη 1
1−p(X) = 1− = = , η = β +β X.
1+eη 1+eη 1+eη 0 1
Taking the ratio:
eη
p(X)
= 1+eη = eη = eβ0+β1X.
1−p(X) 1
1+eη
This yields:
p(X)
= eβ0+β1X, (4.3)
1−p(X)
the odds form of the logistic model.
For the reverse direction, assume (4.3):
p(X)
= eβ0+β1X ≡ r.
1−p(X)
Solving for p(X):
r
p(X) = r(1−p(X)) ⇒ p(X) = r−rp(X) ⇒ p(X)(1+r) = r ⇒ p(X) = .
1+r
1
Substituting r = eβ0+β1X gives:
eβ0+β1X
p(X) = ,
1+eβ0+β1X
which is identical to (4.2).
Thus, the logistic and odds representations are equivalent.
Task 2:
Claim. Under the model X | Y = k ∼ N(µ ,σ2) with priors π , classifying x to the
k k
class that maximizes the posterior probability p (x) in (4.17) is equivalent to classifying
k
x to the class that maximizes
µ µ2
δ (x) = k x− k +logπ (4.18)
k σ2 2σ2 k
Proof. From Bayes’ rule and the normal density with common variance,
(cid:16) (cid:17)
π √ 1 exp −(x−µ k )2
k 2πσ 2σ2
p (x) = . (4.17)
k (cid:16) (cid:17)
(cid:80)K π √ 1 exp −(x−µ ℓ )2
ℓ=1 ℓ 2πσ 2σ2
Maximizing p (x) over k is unaffected by any positive monotone transform and by factors
k
independent of k; hence it is equivalent to maximizing
(cid:16) (cid:16) (cid:17)(cid:17) (x−µ )2
log π exp −(x−µ k )2 = logπ − k .
k 2σ2 k 2σ2
Expand the square:
(x−µ )2 x2 −2xµ +µ2 x2 µ µ2
− k = − k k = − + k x− k .
2σ2 2σ2 2σ2 σ2 2σ2
The term −x2/(2σ2) does not depend on k and therefore does not affect the maximizer.
Discarding this common term yields the discriminant function
µ µ2
δ (x) = k x− k +logπ ,
k σ2 2σ2 k
so the class maximizing p (x) is precisely the class maximizing δ (x).
k k
Task 3:
Proof: Consider K classes with
X | Y = k ∼ N(µ ,σ2),
k k
2
and prior probabilities π . The class-conditional density is
k
1
(cid:18)
(x−µ
)2(cid:19)
f (x) = √ exp − k . (4.16)
k 2πσ 2σ2
k k
By Bayes’ theorem, the posterior probability is
π f (x)
k k
p (x) = .
k (cid:80)K
π f (x)
ℓ=1 ℓ ℓ
Maximizing p (x) over k is equivalent to maximizing π f (x), since the denominator is
k k k
constant with respect to k. Taking the natural log:
√ (x−µ )2
k
logπ f (x) = logπ −log( 2πσ )− .
k k k k 2σ2
k
Expanding the quadratic term:
(x−µ )2 x2 −2xµ +µ2 x2 µ µ2
− k = − k k = − + k x− k .
2σ2 2σ2 2σ2 σ2 2σ2
k k k k k
Thus the discriminant function is
x2 µ µ2
δ (x) = − + k x− k −logσ +logπ .
k 2σ2 σ2 2σ2 k k
k k k
The presence of the x2 term with coefficient − 1 , which depends on k when σ2 varies
2σ2 k
k
across classes, implies that the decision boundaries between classes are determined by
quadratic equations in x.
Therefore, the Bayes classifier in this case is quadratic, not linear.
The classifier is quadratic in x when σ2 differs across classes.
k
Task 4:
(a) For p = 1, X ∼ Uniform(0,1). Using observations within 10% of the range of X
around the test point means using an interval of length 0.1. The proportion of the
dataset used is therefore:
length of interval 0.1
= = 0.10.
total range 1
Thus, on average, 10% of the observations are used.
(b) For p = 2, (X ,X ) ∼ Uniform([0,1] × [0,1]). For each feature, a 10% range is
1 2
used, so the fraction used per dimension is 0.1. Since the features are independent, the
joint proportion is:
0.1×0.1 = 0.01.
3
Thus, on average, only 1% of the observations are used.
(c) For p = 100, each feature uses 0.1 of its range, so:
fraction = (0.1)100.
This number is extremely small:
(0.1)100 ≈ 10−100,
which is effectively zero in practice.
(d) The results show that as p increases, the fraction of observations “near” the
test point decreases exponentially. When p is large, even a small neighborhood in each
coordinate contains almost no points. This is the curse of dimensionality and explains
why KNN and other local methods perform poorly for large p: there are very few nearby
observations to use for prediction.
(e) Suppose the goal is to include 10% of the total data in a p-dimensional hypercube
centered at the test point. If the side length of the hypercube is L, then the fraction of
data it contains is:
Lp = 0.10.
Thus:
L = (0.10)1/p.
For:
p = 1 : L = 0.10,
p = 2 : L = (0.10)1/2 ≈ 0.316,
p = 100 : L = (0.10)1/100 ≈ 0.977.
When p is large, L approaches 1, meaning almost the entire range of each feature must
be used to capture even 10% of the data. This illustrates that in high dimensions, “local”
neighborhoods are not truly local.
Task 5:
1. Linear Bayes boundary:
• Training set: QDA will typically perform better due to its greater flexibility,
which allows it to achieve a smaller training error.
• Test set: LDA will generally perform better because it correctly models the
linear decision boundary and has lower variance, leading to better generaliza-
tion.
4
2. Non-linear Bayes boundary:
• Training set: QDA will again perform better due to its flexibility in capturing
the non-linear nature of the decision boundary.
• Test set: QDA tends to outperform LDA if the sample size is sufficiently large;
however, with small n, LDA may perform comparably or even better due to
QDA’s higher variance.
3. Effect of increasing n: Asthesamplesizenincreases,thetestpredictionaccuracy
of QDA relative to LDA is expected to improve, because the variance of QDA’s
parameter estimates decreases, allowing its lower bias (in non-linear cases) to be
more beneficial.
4. True or False statement:
False. If the Bayes decision boundary is truly linear, LDA is correctly specified
and has fewer parameters, resulting in lower variance. Although QDA can model a
linear boundary, it incurs unnecessary variance at finite n, which usually leads to
worse test performance compared to LDA.
Task 6:
Given: Logistic model
1
ˆ ˆ ˆ
Pr(Y = 1 | X ,X ) = , β = −6, β = 0.05, β = 1.
1 2 0 1 2
1+exp[−(β +β X +β X )]
0 1 1 2 2
(a) For X = 40 (hours) and X = 3.5 (GPA),
1 2
η = −6+0.05(40)+1(3.5) = −6+2+3.5 = −0.5,
1 1
pˆ= = ≈ 0.378.
1+e−η 1+e0.5
So the estimated probability is 0.378.
(b) For a 50% chance (pˆ= 0.5), we need η = 0:
2.5
0 = −6+0.05h+1(3.5) =⇒ 0.05h = 2.5 =⇒ h = = 50.
0.05
So the student needs to study 50 hours.
5
Task 7:
Given: Let Y ∈ {Yes,No} indicate paying a dividend this year, and X be last year’s %
profit.
π ≡ Pr(Y = Yes) = 0.8, Pr(Y = No) = 0.2,
X | Y = Yes ∼ N(µ = 10,σ2 = 36), X | Y = No ∼ N(µ = 0,σ2 = 36).
1 0
We want Pr(Y = Yes | X = 4):
πf (4)
X|Y=Yes
Pr(Y = Yes | X = 4) = .
πf (4)+(1−π)f (4)
X|Y=Yes X|Y=No
1
(cid:18) (x−µ)2(cid:19)
With f(x | µ,σ2) = √ exp − and σ = 6,
2πσ 2σ2
1
(cid:18) (4−10)2(cid:19)
1 1
f ≡ f(4 | 10,36) = √ exp − = √ e−36/72 = √ e−1/2 ≈ 0.04033,
1
2π6 72 2π6 2π6
1
(cid:18) (4−0)2(cid:19)
1 1
f ≡ f(4 | 0,36) = √ exp − = √ e−16/72 = √ e−2/9 ≈ 0.05324.
0
2π6 72 2π6 2π6
Hence,
0.8·0.04033
Pr(Y = Yes | X = 4) = ≈ 0.752.
0.8·0.04033+0.2·0.05324
Task 8:
Given: Logistic regression gives 20% training error and 30% test error. 1-NN (i.e.,
K = 1) reports an average error over training and test of 18%.
Key fact. For K = 1, the training error is (essentially) 0% because each point is its
own nearest neighbor.
train err+test err
Avg. error = = 0.18 ⇒ test err = 2×0.18−0 = 0.36 = 36%.
2
Prefer logistic regression for new observations, since its test error (30%) is lower than
theimplied 1-NNtesterror(36%). Intuitively, 1-NNhasextremelylow biasbutveryhigh
variance (overfits), while logistic regression has higher bias but lower variance, yielding
better generalization here.
6
Task 9:
(a) Odds are related to probability via
p odds
odds = ⇒ p = .
1−p 1+odds
For odds = 0.37,
0.37 0.37
p = = ≈ 0.2701.
1+0.37 1.37
So about 27.0% will default.
(b) Given probability p = 0.16, the odds are
0.16 0.16
odds = = ≈ 0.1905.
1−0.16 0.84
So the odds are approximately 0.19.
Task 10:
For p = 1, the covariance matrix Σ reduces to the scalar variance σ2, and the means
µ ,µ are scalars.
k K
From (4.32),
Pr(Y = k | X = x) π f (x)
k k
log = log .
Pr(Y = K | X = x) π f (x)
K K
The class-conditional densities are
1
(cid:18)
(x−µ
)2(cid:19)
1
(cid:18)
(x−µ
)2(cid:19)
f (x) = √ exp − k , f (x) = √ exp − K .
k 2πσ 2σ2 K 2πσ 2σ2
Thus,
π f (x) π (x−µ )2 (x−µ )2
k k k k K
log = log − + .
π f (x) π 2σ2 2σ2
K K K
Expanding the squares:
x2 −2xµ +µ2 x2 −2xµ +µ2 −x2 +2xµ −µ2 +x2 −2xµ +µ2
− k k + K K = k k K K.
2σ2 2σ2 2σ2
The x2 terms cancel, leaving:
2x(µ −µ )+(µ2 −µ2)
k K K k .
2σ2
7
Therefore,
Pr(Y = k | X = x) π µ2 −µ2 µ −µ
log = log k + K k + k K x.
Pr(Y = K | X = x) π 2σ2 σ2
K
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a k b k1
π µ2 −µ2 µ −µ
a = log k + K k, b = k K .
k π 2σ2 k1 σ2
K
Task 11:
Setup: For QDA, assume
X | Y = k ∼ N(µ ,Σ ), X | Y = K ∼ N(µ ,Σ ), Pr(Y = k) = π , Pr(Y = K) = π .
k k K K k K
Then
p p p
Pr(Y = k | X = x) π f (x) (cid:88) (cid:88)(cid:88)
k k
log = log = a + b x + c x x ,
k kj j kjℓ j ℓ
Pr(Y = K | X = x) π f (x)
K K
j=1 j=1 ℓ=1
where the coefficients come from expanding
π 1 1 1 1
log k − log|Σ |+ log|Σ |− (x−µ )⊤Σ−1(x−µ )+ (x−µ )⊤Σ−1(x−µ ).
π 2 k 2 K 2 k k k 2 K K K
K
Coefficients: Let
(cid:0) (cid:1)
C ≡ 1 Σ−1 −Σ−1 , b ≡ Σ−1µ −Σ−1µ .
k 2 K k k k k K K
Then
(cid:2) (cid:3)
c = (C ) = 1 (Σ−1) −(Σ−1) ,
kjℓ k jℓ 2 K jℓ k jℓ
(cid:0) (cid:1)
b = (b ) = Σ−1µ −Σ−1µ ,
kj k j k k K K j
π
a = log k − 1 log|Σ |+ 1 log|Σ |− 1µ⊤Σ−1µ + 1µ⊤Σ−1µ .
k π 2 k 2 K 2 k k k 2 K K K
K
Note: If the statement uses b instead of c for the quadratic terms, simply b =
kjℓ kjℓ kjℓ
c should be taken as defined above.
kjℓ
Task 12:
Setup: Two classes: {apple,orange}. Your (binary) logistic model:
eβ0+β1x
P(cid:99)r(Y = orange | X = x) = .
1+eβ0+β1x
8
Friend’s softmax (multinomial) model (two-class case):
eαorange,0+αorange,1x
P(cid:99)r(Y = orange | X = x) = .
eαorange,0+αorange,1x +eα apple,0 +α apple,1 x
(a) Log-odds in model:
Pr(Y = orange | x)
log = β +β x.
0 1
Pr(Y = apple | x)
(b) Log-odds in friend’s model:
Pr(Y = orange | x)
(cid:0) (cid:1) (cid:0) (cid:1)
log = α −α + α −α x.
orange,0 apple,0 orange,1 apple,1
Pr(Y = apple | x)
ˆ ˆ
(c) Relating parameters when β = 2, β = −1. Only differences matter in
0 1
softmax (identifiability). They must satisfy
α −α = 2, α −α = −1.
orange,0 apple,0 orange,1 apple,1
One concrete choice (using apple as baseline) is α = 0, α = 0, α =
apple,0 apple,1 orange,0
2, α = −1. Any common shift (c ,c ) added to both classes leaves probabil-
orange,1 0 1
ities unchanged.
(d) Given friend’s estimates α = 1.2, α = −2, α = 3, α =
orange,0 orange,1 apple,0 apple,1
0.6. Logistic coefficients are the differences:
ˆ ˆ
β = α −α = 1.2−3 = −1.8, β = α −α = −2−0.6 = −2.6.
0 orange,0 apple,0 1 orange,1 apple,1
(e) Agreement on a test set: With two classes, softmax reduces exactly to logistic
withβ = α −α andβ = α −α . Hencebothmodelscompute
0 orange,0 apple,0 1 orange,1 apple,1
identical probabilities for every x and therefore make the same class prediction for
every observation. The fraction of agreement is 100% (i.e., all 2000/2000).
1.2 Applied:
Task 13:
Task is done using Weekly dataset, which contains 1089 rows and 9 columns and total 8
numeric columns, 1 categoric column.
a.
9
The pairplot illustrates the relationships among the variables Year, Lag1–Lag5, Vol-
ume, and Today, with data points colored according to the Direction variable (Up or
Down). The Year variable spans approximately from 1990 to 2010 and is distinct from
the other predictors, showing a clear time progression. The lag variables (Lag1–Lag5)
and Today are centered around zero and exhibit low visible correlation with one an-
other, as indicated by the scattered, cloud-like distributions in the scatterplots. Volume
stands out with a positively skewed distribution and a noticeable upward trend over time.
Across all plots, there is no strong visual separation between the Up and Down classes,
suggesting that individual predictors may have limited classification power for Direction.
However, potential patterns could still emerge through the use of multivariate modeling
or transformations, as simple visual inspection does not capture all possible predictive
relationships.
b.
10
The logistic regression model was fit using Direction as the response and the five
lag variables plus Volume as predictors. The results indicate that most predictors have
p-values greater than 0.05, suggesting they are not statistically significant at the 5%
significance level. The only variable showing statistical significance is Lag2 (p = 0.030),
which has a positive coefficient, implying that higher values of Lag2 are associated with
a greater probability of the market moving Up. All other predictors, including Lag1,
Lag3, Lag4, Lag5, and Volume, do not show statistically significant relationships with
the response variable in this model.
c.
11
The confusion matrix shows that the logistic regression model correctly predicted 54
Down days and 557 Up days, while it incorrectly classified 430 Down days as Up and 48
Up days as Down. This indicates that the model is strongly biased toward predicting
the “Up” class, as most Down days are being misclassified. The overall accuracy is high
because the dataset appears imbalanced toward the Up class, but the poor performance
in correctly identifying Down days suggests that the model is not balanced in predictive
power across both categories. This means it performs well in predicting Up days but
struggles to capture Down days effectively.
d.
The logistic regression model with Lag2 achieved 62.5% accuracy on the 2009–2010
test data, correctly predicting most Up days but misclassifying the majority of Down
days as Up.
e.
The LDA model with Lag2 achieved an accuracy of 62.5% on the 2009–2010 test data,
identical to the logistic regression result. It correctly predicted 9 Down days and 56 Up
12
days, but misclassified 34 Down days as Up and 5 Up days as Down, showing the same
bias toward predicting Up movements.
f.
The QDA model with Lag2 achieved 58.7% accuracy, predicting all days as Up. It
correctly identified all Up days but failed to classify any Down days.
g.
The KNN model with K=1 and Lag2 achieved 49.0% accuracy on the 2009–2010 test
data. It correctly classified 22 Down days and 31 Up days but misclassified 21 Down days
as Up and 30 Up days as Down, indicating performance close to random guessing.
h.
13
The Naive Bayes model with Lag2 achieved 58.7% accuracy on the 2009–2010 test
data. It predicted every observation as “Up,” correctly classifying all Up days (61) but
failing to identify any Down days, showing a complete bias toward the Up class.
i.
Among the tested models, logistic regression and LDA with Lag2 both achieved the
highest accuracy at 62.5% on the 2009–2010 test data. They also correctly classified some
Down days, unlike QDA and Naive Bayes, which predicted all observations as Up. While
KNN had more balanced predictions between classes, its overall accuracy was lower at
49%. Therefore, logistic regression or LDA appear to provide the best results for this
dataset, with similar performance.
Task 15:
a.
def Power():
result = 2 ** 3
print(result)
b.
def Power2(x, a):
result = x ** a
print(result)
c.
Power2(10, 3)
Power2(8, 17)
Power2(131, 3)
d.
14
def Power3(x, a):
result = x ** a
return result
value = Power3(3, 4)
print(value)
e.
f.
def PlotPower(x_values, a):
y_values = [x ** a for x in x_values]
plt.plot(x_values, y_values, marker=’o’, label=fr"$x^{a}$")
plt.xlabel("x")
plt.ylabel(fr"$x^{a}$")
plt.title(fr"Plot of $x$ vs $x^{a}$")
plt.legend()
plt.grid(True)
plt.show()
PlotPower(np.arange(1, 11), 3)
15
2 Ch. 5: Resampling Methods
2.1 Conceptual:
Task 1:
Goal: Minimize Var (cid:0) αX +(1−α)Y (cid:1) over α ∈ R.
(cid:0) (cid:1)
Var αX +(1−α)Y = α2σ2 +(1−α)2σ2 +2α(1−α)σ ,
X Y XY
where σ2 = Var(X), σ2 = Var(Y), and σ = Cov(X,Y).
X Y XY
Expand:
V(α) = α2σ2 +(1−2α+α2)σ2 +2ασ −2α2σ .
X Y XY XY
Collect terms in α:
V(α) = (σ2 +σ2 −2σ )α2 +(−2σ2 +2σ )α+σ2.
X Y XY Y XY Y
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
A B
Differentiate and set to zero:
B σ2 −σ
V′(α) = 2Aα+B = 0 =⇒ α∗ = − = Y XY .
2A σ2 +σ2 −2σ
X Y XY
Second derivative:
V′′(α) = 2A = 2(σ2 +σ2 −2σ ) ≥ 0
X Y XY
(with equality only if X −Y is degenerate), hence α∗ is a minimizer.
σ2 −σ
α∗ = Y XY
σ2 +σ2 −2σ
X Y XY
Task 2:
(a) The probability that the first bootstrap observation is not the jth observation from
the original sample is
1
1− .
n
(b) The probability that the second bootstrap observation is not the jth observation
from the original sample is also
1
1− ,
n
since each bootstrap draw is independent.
16
(c) The probability that the jth observation is not in the bootstrap sample is
(cid:18)
1
(cid:19)n
1− ,
n
since all n draws must avoid the jth observation.
(d) For n = 5, the probability that the jth observation is in the bootstrap sample is
(cid:18)
1
(cid:19)5 (cid:18)
4
(cid:19)5
1− 1− = 1− ≈ 0.67232.
5 5
(e) For n = 100, the probability that the jth observation is in the bootstrap sample is
(cid:18)
1
(cid:19)100
1− 1− ≈ 0.63397.
100
(f) For n = 10,000, the probability that the jth observation is in the bootstrap sample
is
(cid:18)
1
(cid:19)10000
1− 1− ≈ 0.63214.
10000
Task 3:
(a) Implementation of k-fold cross-validation: The dataset is randomly parti-
tioned into k approximately equal-sized folds. For each of the k iterations:
• One fold is held out as the validation set.
• The model is trained on the remaining k −1 folds.
• The model performance is evaluated on the held-out fold.
The k performance estimates are then averaged to produce an overall performance
metric.
(b) Advantages and disadvantages:
i. Relative to the validation set approach:
• Advantages: Reduces variance in the performance estimate by averaging
over multiple folds; uses the entire dataset for both training and validation
across iterations.
• Disadvantages: Computationally more expensive than a single valida-
tion set split.
ii. Relative to LOOCV (Leave-One-Out Cross-Validation):
17
• Advantages: Less computationally expensive; lower variance in the per-
formance estimate since each training set is less similar to the full dataset.
• Disadvantages: Slightly higher bias in the performance estimate com-
pared to LOOCV, since the training sets are smaller.
Task 4:
Toestimatethestandarddeviationofthepredictionforaparticularvalueofthepredictor
X:
1. Resampling approach: Use a method such as the bootstrap.
• Generate many bootstrap samples from the original dataset by sampling with
replacement.
• For each bootstrap sample, fit the statistical learning method to the resampled
data.
• Use the fitted model to predict the response Y at the given value of X.
• Collect the set of predicted values from all bootstrap replications.
2. Estimate variability: Compute the sample standard deviation of the bootstrap
predictions. This standard deviation provides an estimate of the standard deviation
(uncertainty) of the prediction at the specified X.
2.2 Applied:
Task 5:
a.
X = df[[’income’, ’balance’]]
y = (df[’default’] == ’Yes’).astype(int)
model = LogisticRegression(solver=’liblinear’)
model.fit(X, y)
print("Model coefficients:", model.coef_)
print("Intercept:", model.intercept_)
b. Validation Error (split 1): 0.0322
c.
A logistic regression model was fit to predict default using income and balance as
predictors. Model performance was evaluated using the validation set approach, where
18
the dataset was split into training and validation sets (50% each). For part (c), this
procedure was repeated three times with different random splits.
The validation errors obtained were:
• Random split 1: 3.22%
• Random split 2: 3.12%
• Random split 3: 3.40%
The average validation error across the three splits was 3.25%, corresponding to an
average accuracy of 96.75%. The small variation in error rates across different splits
indicates that the model’s performance is stable and not highly sensitive to the specific
partitioning of the dataset. These results suggest that income and balance are strong
predictors of credit default in the Default dataset.
3 Ch 6. Linear Model Selection and Regularization
3.1 Conceptual:
Task 1:
(a) The “best subset selection” method will yield the smallest training residual sum
of squares (RSS) because it evaluates all possible models and chooses the one with the
lowest training RSS. This exhaustive approach ensures the minimal possible training RSS
compared to other methods.
(b) It is not possible to determine in advance which method will produce the lowest
test set RSS (i.e., the lowest prediction error). The “best subset selection” method may
overfit the data, resulting in a low training RSS but potentially a higher test RSS due to
high variance and low bias. For the other methods, indirect evaluation techniques, such
as cross-validation or information criteria, are necessary to identify the approach that
minimizes the test RSS.
(c)
i. True: By definition, the (k + 1)-variable model contains the predictors from the
k-variable model as a subset.
ii. False: In the (k +1) step of backward stepwise selection, a predictor is removed,
so the k-variable model is not a subset of the (k +1)-variable model.
iii. False: Consider a simple case with variables X ,X , and X . In backward stepwise
1 2 3
selection with k = 1, X may be removed if deemed least important. However, in
3
forward stepwise selection with k = 1, X might be chosen if it alone explains the
3
19
data best. Thus, the backward selection model is not necessarily a subset of the
forward selection model.
iv. False: The reasoning is the same as in (iii); the forward and backward selection
paths may differ, so one is not necessarily a subset of the other.
v. False: This assumes that forward stepwise selection will always choose the best
predictors. However, best subset selection may not include the first variable chosen
by forward selection.
Task 2:
(a) Lasso Regularization True statement: Less flexible, and therefore can yield
improved prediction accuracy when the increase in bias is outweighed by the reduction
in variance.
(b) Ridge Regularization True statement: Less flexible, and therefore can yield
improved prediction accuracy when the increase in bias is outweighed by the reduction
in variance.
Task 5:
Setup: n = 2, p = 2 with x = x , x = x and y +y = 0, x +x = 0, x +x =
11 12 21 22 1 2 11 21 12 22
ˆ
0. Hencetheinterceptestimateisβ = 0andthetwopredictorcolumnsareidentical. Let
0
x = (x ,x )⊤ and y = (y ,y )⊤; then the model is y ≈ (β +β )x. Denote t ≡ β +β
11 21 1 2 1 2 1 2
and s ≡ x⊤x, b ≡ y⊤x.
(a) Ridge optimization problem:
2
min
(cid:88)(cid:0)
y −β x −β x
(cid:1)2
+ λ
(cid:0)
β2 +β2
(cid:1)
.
i 1 i1 2 i2 1 2
β1,β2
i=1
ˆ ˆ
(b) Ridge solution satisfies β = β . Because x = x , the residual sum of squares
1 2 i1 i2
depends on (β +β ) only:
1 2
RSS(β ,β ) = ∥y −tx∥2 = y⊤y −2tb+t2s.
1 2
For fixed t, the ridge penalty is minimized when β = β = t/2 (by convexity or by
1 2
Cauchy–Schwarz/AM–QM). Hence the ridge estimator enforces equality:
tˆ 2b
β ˆ = β ˆ = , tˆ= .
1 2
2 2s+λ
20
(c) Lasso optimization problem:
2
(cid:88)(cid:0) (cid:1)2 (cid:0) (cid:1)
min y −β x −β x + λ |β |+|β | .
i 1 i1 2 i2 1 2
β1,β2
i=1
(d) Lasso solutions are non-unique. Again RSS depends only on t = β +β :
1 2
(cid:0) (cid:1)
Objective(t,β ,β ) = y⊤y −2tb+t2s + λ |β |+|β | subject to β +β = t.
1 2 1 2 1 2
For any fixed t ̸= 0, the minimum of |β |+|β | under β +β = t equals |t| and is
1 2 1 2
attained by any pair with the same sign as t (e.g., (t,0), (0,t), or (θt,(1−θ)t) with
θ ∈ [0,1]). Therefore the lasso reduces to a one-parameter problem in t,
min y⊤y −2tb+t2s+λ|t|,
t
which has a unique tˆ(soft-thresholded at λ/2), but yields infinitely many coefficient
pairs:
{(β ˆ ,β ˆ ) : β ˆ +β ˆ = tˆ, β ˆ β ˆ ≥ 0} .
1 2 1 2 1 2
When tˆ= 0, the unique lasso solution is (β ˆ ,β ˆ ) = (0,0).
1 2
Task 6:
Goal: Verify the minimizers of
Ridge: J (β ) = (y −β )2 +λβ2 (6.12,p=1)
R 1 1 1 1
Lasso: J (β ) = (y −β )2 +λ|β | (6.13,p=1)
L 1 1 1 1
and confirm (6.14) and (6.15).
(a) Ridge, p = 1.
J′ (β ) = −2(y −β )+2λβ ⇒ −2y +2(1+λ)β = 0
R 1 1 1 1 1 1
y
⇒ β
ˆR
=
1
, (6.14)
1 1+λ
and J′′(β ) = 2(1+λ) > 0 confirms a global minimum. A plot of J (β ) against β is a
R 1 R 1 1
convex parabola with its vertex at y /(1+λ).
1
(b) Lasso, p = 1. Treat the subproblems for β > 0, β < 0, and β = 0.
1 1 1
β > 0: J (β ) = (y −β )2 +λβ ⇒ J′(β ) = −2(y −β )+λ = 0 ⇒ β = y − λ.
1 L 1 1 1 1 L 1 1 1 1 1 2
Feasible only if y > λ/2.
1
21
β < 0: J (β ) = (y −β )2 −λβ ⇒ J′(β ) = −2(y −β )−λ = 0 ⇒ β = y + λ.
1 L 1 1 1 1 L 1 1 1 1 1 2
Feasible only if y < −λ/2.
1
β = 0: If |y | ≤ λ/2, the left and right derivatives bracket 0, so β = 0 minimizes J .
1 1 1 L
Thus
 λ λ
y − , y > ,
 1 1
 2 2


 λ λ
β
ˆL
= y + , y < − , (6.15)
1 1 1
2 2


  λ
 0, |y | ≤ ,
1
2
i.e., soft-thresholding at λ/2. A plot of J (β ) shows a V-shaped kink at β = 0; the
L 1 1
minimizer sits at the soft-thresholded value above.
3.2 Applied:
Task 8:
# Generate the simulated data
X = np.random.randn(100)
err = np.random.randn(100)
# Set the estimators
beta_0 = 10
beta_1 = 20
beta_2 = 20
beta_3 = 3
# Generate the response
Y = beta_0 + beta_1*X + beta_2*X**2 + beta_3*X**3 + err
Since Python does not provide a built-in function for best subset selection, a custom
functionbsswasimplemented. Givenasetofpredictors,thisfunctionreturnstheoptimal
subset according to a specified performance metric.
Applying this function to predictors X raised to powers up to 10 yielded the following
results:
• Best subset for Adjusted R2: (X ,X ,X ,X ,X )
1 2 5 7 9
• Best subset for BIC: (X ,X ,X ,X ,X )
1 2 5 7 9
• Best subset for AIC: (X ,X ,X ,X ,X )
1 2 5 7 9
22
For this particular random seed, the same subset of predictors was selected for all
three criteria. However, this outcome is not ideal, as the true underlying relationship
only involves terms up to the cubic power. The fitted model appears overly flexible,
capturing noise in the data, and may therefore exhibit poor generalization performance
on an independent test set.
23