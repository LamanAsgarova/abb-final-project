Probability of Default (PD) Modeling –
Loan Risk Assessment
Laman Asgarova
Laman.Asgarova@abb-bank.az
August 15, 2025
1 Introduction
Loan default poses a substantial challenge for financial institutions, directly impacting
profitability, capital adequacy, and compliance with regulatory requirements. Effective
prediction of a borrower’s likelihood to default enables banks to make informed credit
decisions, optimize lending terms, and proactively manage portfolio risk. Probability of
Default (PD) modeling provides a data-driven approach to quantify default risk, allowing
institutions to implement risk-based pricing, targeted interventions, and portfolio opti-
mization strategies.
This report outlines the development of a binary classification machine learning model
to estimate the probability that a loan applicant will default, using demographic, finan-
cial, and credit history data. The process encompasses data preparation, exploratory
analysis, feature engineering, model building, performance evaluation, calibration, and
simulation of risk-based lending strategies. The ultimate goal is to deliver a robust, inter-
pretable, and calibrated PD model that supports strategic decision-making and enhances
the bank’s credit risk management framework.
2 Dataset
The dataset used in this task is custom bank dataset with applicant demographics, credit
score, financials, loan details, credit history details and target column indicating if cus-
tomer is defaulted or not. Columns are described below in detail:
• age: Borrower’s age (22–65), simulated using a beta distribution to reflect a skewed
adult population.
• gender: Randomly assigned gender with realistic probabilities.
1
• marital status: Random marital status: Single, Married, Divorced, weighted to-
ward Married and Single.
• employment status: Employment status chosen with realistic distribution: Em-
ployed (70%), Self-Employed (25%), Unemployed (5%).
• housing status: Simulated housing condition, with random mix and inconsistent
casing for added data cleaning challenges.
• credit score: Normally distributed credit score (mean=700, std=85), clipped be-
tween 300 and 850 to match real FICO range.
• income: Annual income drawn from a log-normal distribution, based on state
averages and adjusted for employment status.
• expenses: Monthly expenses calculated as a ratio of income, using a beta distri-
bution to reflect varying spending habits.
• loan amount: Loan size determined as a proportion of income plus a base noise
term, capped between $500 and $75,000.
• debt to income: DTI ratio computed as (expenses + noise) / monthly income,
capped between 0.05 and 0.6; set to 0.6 for unemployed.
• term: Loan term randomly selected as either 36 or 60 months, with a 70% prefer-
ence for shorter term.
• interest rate: Base rate tied to credit score and economic cycle, with random
noise added, bounded between 4.5% and 29%.
• collateral: Binary indicator whether loan is collateralized (1 = Yes, 0 = No), with
20% chance of having collateral.
• loan purpose: Purposeoftheloan(e.g.,debt consolidation,credit card,small business),
sampled with realistic frequencies.
• loan title: Text description generated using Faker and loan purpose, mimicking
free-text input in real applications.
• delinquencies: Number of past credit delinquencies, sampled from a Poisson dis-
tribution with λ = 0.8.
• inquiries: Count of recent credit inquiries, also sampled from Poisson(1.5), repre-
senting recent credit-seeking activity.
2
• credit history length months: Random number of months since credit history
started, capped by age to keep logic intact.
• application date: Simulated application date between 2023 and 2024, used to
model economic cycles.
• state: U.S. state of residence (CA, NY, TX, FL, IL), randomly sampled with equal
probability.
• default: Binary target (0 = repaid, 1 = defaulted), derived using a weighted sum
of risk features + noise, then sampled as a probability.
In the end, dataset was saved as loan.csv.
3 Data Understanding & Cleaning
1. Dataset loaded using
df=pd.read_csv(’loan.csv’)
2. Dataset consists of 50000 rows, and 22 columns (checked with df.shape).
3. df.info() was used to get initial dataset info:
As seen in the figure, the dataset information includes its size, memory usage, to-
tal non-null values for each column, and their data types. It is evident that the income,
expenses, debt to income, and credit history length months columns contain missing val-
ues, and the data type of application date is object, which was handled as follows:
3
df[’application_date’] = pd.to_datetime(df[’application_date’], errors=’coerce’)
4. df.describe() was used to get statisticval information about numeric values of the
dataset.
Handling missing values:
Columns with missing values and their counts (checked with df.isnull().sum()):
Null values in income, expenses, and credit history length months columns filled with
median of their values:
df[’income’].fillna(df[’income’].median(), inplace=True)
df[’expenses’].fillna(df[’expenses’].median(), inplace=True)
df[’credit_history_length_months’].fillna(df[’credit_history_length_months’].
median(), inplace=True)
debt to income column:
monthly_income = df[’income’] / 12
monthly_income[monthly_income == 0] = np.nan
df[’debt_to_income’] = (df[’expenses’] +
np.random.uniform(1, 500, size=len(df))) / monthly_income
df[’debt_to_income’] = np.clip(df[’debt_to_income’], 0.05, 0.6).round(4)
df[’debt_to_income’].fillna(df[’debt_to_income’].median(), inplace=True)
Thiscodecalculatesthedebt-to-incomeratiobydividingeachindividual’sexpenses(with
a small random addition between 1 and 500) by their monthly income. Monthly income
is derived from annual income, with zero values replaced by NaN to avoid division errors.
The resulting ratios are clipped to a range of 0.05–0.6, rounded to four decimals, and
missing values are replaced with the median ratio.
Remove duplicates:
There were 72 duplicated values in dataset which checked using df.duplicated().sum():
df = df.drop_duplicates(keep=’first’).reset_index(drop=True)
4
Impute or remove outliers:
The Interquartile Range (IQR) method is applied to identify and cap extreme values
without removing them.
Formula for Outlier Boundaries:
IQR = Q −Q
3 1
Lower Bound = Q −1.5×IQR
1
Upper Bound = Q +1.5×IQR
3
Any value falling below the lower bound or above the upper bound is considered an
outlier.
4 Exploratory Data Analysis (EDA)
In this section, summary statistics, distribution plots, and feature relationships are ex-
amined in detail.
Summary statistics:
Here,theloanamount,income,andcreditscorecolumnsaresummarizedusingdf.describe()
toobtainstatisticalinformation, includingtheirminimumandmaximumvalues, quartiles
(Q1, Q2, Q3), interquartile ranges (IQRs), means, and standard deviations (std).
The dataset demonstrates a broad and realistic distribution of key financial features
5
after cleaning. Loan amounts range from $501 to $75,000, with a mean of approximately
$31,051 and a median of $24,501, suggesting a slightly right-skewed distribution. Income
spans from $0 to about $274,762 annually, with a mean of roughly $99,008 and a me-
dian of $78,901. The higher mean relative to the median indicates positive skew due to
high-income borrowers, while zero-income entries likely represent unemployed applicants.
Credit scores range from 471 to 850, with a mean of about 698 and a median of 700,
reflecting a near-normal distribution centered around the “fair to good” risk category.
These figures confirm that the dataset preserves meaningful variability across borrowers,
supporting effective risk segmentation and predictive modeling.
Distribution plots:
This section presents the distribution of key numerical features, including income, loan
term, credit score, and interest rate. Visualizing these distributions helps identify skew-
ness, outliers, and potential segmentation patterns, which are essential for understanding
borrower characteristics and guiding subsequent modeling steps.
Income:
The income distribution is positively skewed, with most borrowers earning between ap-
proximately $20,000 and $100,000 annually. Distinct spikes are observed at $0, $90,000,
and $270,000, suggesting the presence of special categories such as unemployed appli-
cants, capped salary bands, or data entry thresholds. The right tail extends toward
higher incomes, indicating a smaller group of high earners who contribute to the skew-
ness. This variation reflects a diverse borrower base, but the pronounced peaks at specific
6
values may warrant further investigation to determine whether they result from reporting
practices or institutional salary structures.
Loan term:
The loan term distribution includes two available repayment durations, 36 months and
60 months. A larger proportion of borrowers selected the 36 month term, while a smaller
group chose the 60 month option. This reflects common lending practices, where shorter-
term personal loans are more frequently offered or preferred due to lower total interest
costs and quicker repayment periods. The imbalance between the two term lengths also
aligns with the simulation design, in which 36 month terms were assigned with higher
probability. The resulting distribution supports meaningful analysis of borrower behavior
and repayment risk across different loan durations.
7
Credit score:
Thecreditscoredistributionisapproximatelybell-shaped, centeredaroundthe700mark,
indicating that most borrowers fall within the “fair to good” credit range. The spread is
relatively wide, with scores ranging from below 500 to the maximum of 850. A noticeable
peak at 850 suggests a group of borrowers with perfect credit scores, which may reflect
highly creditworthy individuals or scoring system limits. The left tail shows fewer bor-
rowers with poor credit scores below 600, while the bulk of the population lies between
650 and 750, supporting a relatively low-risk borrower profile in the dataset.
Interest rate:
8
The interest rate distribution shows a concentration of borrowers clustered between 6%
and 12%, with the highest density around 8–10%. A pronounced spike at approximately
4.5% suggests the presence of a fixed or promotional rate applied to a specific subset of
loans. The right tail extends toward higher interest rates, reaching up to about 17%,
indicating a smaller segment of high-risk borrowers charged premium rates. Overall, the
distribution reflects a mix of standard market rates, special low-rate offers, and higher
rates for risk-adjusted lending.
Relationships:
Thissubsectionexamineshowborrowerandloancharacteristicsrelatetodefaultrisk. Re-
lationships are analyzed by segmenting default rates across key variables such as credit
score buckets, loan purpose, loan term, and debt-to-income ratio. These insights help
identify patterns, high-risk segments, and potential predictors for the Probability of De-
fault (PD) model.
Default rate by credit score bucket:
The chart shows a clear negative relationship between credit score and default rate. Bor-
rowers with poor credit scores (¡600) have the highest default rate, exceeding 45%, while
those in the excellent range (800+) exhibit the lowest default rate at around 33%. The
trend is consistent across buckets, with default rates gradually declining as creditwor-
thiness improves. This pattern confirms that credit score is a strong predictor of loan
repayment behavior, aligning with established credit risk assessment principles.
9
Default rate by loan purpose:
Thechartindicatesthatdefaultratesvaryacrossloanpurposes, withsmallbusinessloans
exhibiting the highest default rate at over 50%, significantly above other categories. Most
other purposes—such as credit card refinancing, debt consolidation, home improvement,
majorpurchases,andmedicalexpenses—clusterbetween41%and44%,showingrelatively
similar risk levels. The “other” category has the lowest observed default rate, slightly
above 40%. These results suggest that small business lending carries substantially higher
risk and may warrant stricter underwriting or risk-adjusted pricing strategies.
Default rate by DTI ratio:
The boxplot shows that borrowers who default (Default = 1) tend to have higher Debt-
to-Income (DTI) ratios compared to those who do not default (Default = 0). The median
DTI for defaulters is noticeably above that of non-defaulters, and the interquartile range
10
is also shifted upward, indicating a generally higher debt burden among defaulting bor-
rowers. While there is some overlap between the groups, the trend suggests that higher
DTI ratios are associated with increased default risk, making this feature a valuable
predictor in credit risk modeling.
Heatmap of feature correlation:
The heatmap reveals strong positive correlations between income and loan amount (0.85)
as well as income and expenses (0.75), indicating that higher-income borrowers typically
qualify for larger loans and spend more. Credit score has a moderate negative correlation
with interest rate (-0.50), consistent with lower rates for more creditworthy borrowers.
For the target variable (default), the highest correlation is with debt-to-income ratio
(0.16), supporting its role as a key risk indicator. Credit score (-0.08) and income (-
0.09) show weak negative correlations with default, suggesting they contribute to risk
assessment but are not sole determinants.
11
Default rate over time:
The chart shows that the monthly default rate remained relatively stable between 0.37
and 0.41 throughout most of 2023, with minor fluctuations. However, from late 2023
to early 2024, there was a sharp and sustained increase, peaking at nearly 0.48 in early
2024. Although there was a slight decline in mid-2024, default rates stayed elevated,
consistently above 0.45 for the remainder of the year. This trend suggests a potential
shift in economic conditions, lending policies, or borrower profiles that increased default
risk starting in early 2024.
5 Feature engineering
ToenhancethepredictivepoweroftheProbabilityofDefault(PD)model, severalderived
features were created from the raw dataset. These engineered variables aim to capture
borrower risk characteristics and lending patterns more effectively:
• fico bucket – Segments borrowers into credit score categories (¡600, 600–700,
700–800, 800+) to simplify interpretation of risk associated with creditworthiness.
bins = [300, 600, 700, 800, 851]
labels = [’<600 (Poor)’, ’600{700 (Fair)’,
’700{800 (Good)’, ’800+ (Excellent)’]
df[’credit_score_bucket’] = pd.cut(df[’credit_score’],
bins=bins, labels=labels, right=False)
12
• loan to income ratio – Calculates the proportion of the loan amount to the bor-
rower’s annual income, indicating repayment burden relative to earnings.
df[’loan_to_income_ratio’] = df[’loan_amount’] / df[’income’]
• installment to income – Measures the ratio of monthly installment payments to
monthly income, providing a direct assessment of affordability.
df[’installment_to_income’] = (df[’loan_amount’] / df[’term’]) / (df[’income’] / 12)
• has recent delinquency – Binary flag indicating whether the borrower has expe-
rienced any delinquencies within a recent time frame, serving as an early warning
signal for default risk.
df[’has_recent_delinquency’] = (df[’delinquencies’] > 0).astype(int)
• credit history length – Derived in months from the earliest recorded credit line
to the application date, reflecting the borrower’s credit experience.
df[’credit_history_length’] = df[’credit_history_length_months’] / 12
• high risk purpose – Categorical flag identifying loan purposes historically asso-
ciated with higher default rates (e.g., small business loans).
high_risk_categories =
[’small_business’, ’other’, ’medical’, ’major_purchase’]
df[’high_risk_purpose’] =
df[’loan_purpose’].isin(high_risk_categories).astype(int)
These engineered features combine raw financial metrics with behavioral indicators,
improving model interpretability and enabling better risk segmentation.
6 Model building
Before model training, categorical variables were encoded, and numerical features were
scaled to address potential scale disparities. The dataset was then split into training
(80%) and testing (20%) sets using stratified sampling to preserve the default rate dis-
tribution. Two model types were developed: a baseline Logistic Regression for inter-
pretability and a tree-based model (Random Forest, XGBoost, or LightGBM) to capture
complex, non-linear relationships.
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2,
random_state=42, stratify=y)
13
Logistic Regression:
log_reg_model = LogisticRegression(solver=’liblinear’, random_state=42)
log_reg_model.fit(X_train, y_train)
A logistic regression model was configured using the liblinear solver, which is efficient
for small to medium-sized datasets and supports both L1 and L2 regularization. A fixed
random state was applied to ensure reproducibility. This model serves as a baseline
binary classifier, providing interpretable coefficients for understanding the relationship
between predictors and default risk.
LightGBM:
lgbm_model = LGBMClassifier(
n_estimators=100,
learning_rate=0.1,
max_depth=-1,
random_state=42
)
This configuration sets up a LightGBM classifier for the Probability of Default modeling
task. The parameter n estimators=100 specifies that the model will train 100 boosting
iterations (trees). A learning rate of 0.1 controls how much each tree contributes to the
final prediction, balancing model speed and accuracy. The max depth=-1 setting allows
trees to grow without depth limits, enabling the model to capture complex patterns in
the data. Finally, random state=42 ensures reproducibility of results across runs.
7 Evaluation metrics
This section explains the performance metrics used to assess model effectiveness, with a
focus on imbalanced classification scenarios such as loan default prediction.
• AUC-ROC (Area Under the Receiver Operating Characteristic Curve) – Evalu-
ates the model’s ability to distinguish between defaulters and non-defaulters across
different probability thresholds. A higher AUC indicates stronger discriminatory
power.
• Precision – Measures the proportion of correctly predicted defaults out of all
predicted defaults, helping to reduce false positives in risk assessment.
• Recall – Represents the proportion of actual defaults correctly identified, which is
critical for minimizing missed high-risk borrowers.
14
• F1-score – Provides a balanced measure by combining precision and recall, useful
when both false positives and false negatives carry business impact.
• Confusion Matrix – Summarizes prediction results into true positives, false posi-
tives, true negatives, and false negatives, offering a complete performance snapshot.
• KS Statistic (Kolmogorov–Smirnov) – Quantifies the maximum separation be-
tween the cumulative distributions of good and bad borrowers, widely used in credit
risk modeling.
• Precision-Recall Curve – Plots precision against recall for different thresholds,
offering insights into performance under imbalanced conditions.
• Threshold Tuning –Involvesadjustingtheprobabilitycut-offtoalignthemodel’s
predictions with business objectives, such as controlling default rates or optimizing
approval volumes.
Logistic Regression:
• AUC-ROC Score for Logistic regression is 0.6659.
• ROC curve:
The ROC curve for the logistic regression model plots the True Positive Rate (TPR)
against the False Positive Rate (FPR) across various classification thresholds. The blue
curve lies above the red diagonal reference line, which represents random guessing, indi-
cating that the model has some discriminatory power. However, the curve’s proximity to
15
the diagonal suggests that the model’s ability to separate defaulters from non-defaulters
is moderate, with significant room for improvement. The AUC (Area Under the Curve)
value, though not shown here, would quantify this performance, with higher values re-
flecting stronger predictive capability.
• F1 Score: 0.46533036377134374
• Precision: 0.6290646326776395
• Recall: 0.36922714420358155
• Confusion matrix:
Theconfusionmatrixforthelogisticregressionmodelshowstheclassificationresults
for the test set. Out of all non-defaulters (0), the model correctly predicted 4,818
(TrueNegatives)andmisclassified924asdefaulters(FalsePositives). Fordefaulters
(1), the model correctly identified 1,567 (True Positives) but missed 2,677 cases,
predicting them as non-defaulters (False Negatives). While the model performs
reasonably well in identifying non-defaulters, the higher number of false negatives
indicates a limitation in capturing a substantial portion of actual default cases,
which is critical in credit risk applications.
• Classification report:
16
The classification report for the logistic regression model indicates mixed perfor-
mance across classes. For non-defaulters (0), the model achieves a precision of
0.6428 and a recall of 0.8391, resulting in an F1-score of 0.7280, showing strong
ability to correctly identify non-defaulters. For defaulters (1), precision is 0.6291
but recall drops to 0.3692, with an F1-score of 0.4653, highlighting difficulty in cap-
turing actual default cases. Overall accuracy is 63.94%, while macro and weighted
averages for F1-score are around 0.60–0.62, suggesting that although the model
is reasonably balanced in precision, its recall for the default class is notably low,
limiting its effectiveness in risk detection.
• Precision-Recall Curve:
The Precision-Recall curve for the logistic regression model shows the trade-off be-
tween precision and recall across different probability thresholds. Precision remains
high at low recall values, indicating strong accuracy when the model is confident
in predicting defaults. However, as recall increases, capturing more actual default
cases, precision declines steadily, reflecting the rise in false positives. The curve’s
downward slope highlights the inherent balance between identifying more default-
ersandmaintainingpredictionaccuracy, akeyconsiderationinimbalanceddatasets
like credit default prediction.
• KS statistic: 0.2400 - indicates moderate discriminatory power, meaning the
model can separate defaulters from non-defaulters with a maximum distribution
difference of 24%.
• Threshold tuning:
17
Table 1: Logistic Regression Performance Across Thresholds
Threshold Class Precision Recall F1-score Support Accuracy Macro F1
0.1 0 0.0000 0.0000 0.0000 5742
0.4250 0.2982
1 0.4250 1.0000 0.5965 4244
0.2 0 0.8553 0.0350 0.0673 5742
0.4417 0.3345
1 0.4318 0.9920 0.6016 4244
0.3 0 0.7389 0.2588 0.3833 5742
0.5212 0.4960
1 0.4663 0.8763 0.6087 4244
0.4 0 0.6880 0.5883 0.6342 5742
0.6099 0.6081
1 0.5343 0.6390 0.5820 4244
0.5 0 0.6428 0.8391 0.7280 5742
0.6394 0.5966
1 0.6291 0.3692 0.4653 4244
0.6 0 0.6101 0.9575 0.7453 5742
0.6238 0.5127
1 0.7497 0.1722 0.2801 4244
0.7 0 0.5976 0.9871 0.7445 5742
0.6105 0.4624
1 0.8526 0.1008 0.1804 4244
0.8 0 0.5956 0.9909 0.7440 5742
0.6080 0.4535
1 0.8799 0.0898 0.1629 4244
0.9 0 0.5819 0.9990 0.7354 5742
0.5866 0.3956
1 0.9531 0.0287 0.0558 4244
The threshold significantly impacts the trade-off between precision and recall. Very low
thresholds (0.1–0.3) maximize recall for defaulters but severely reduce precision for non-
defaulters, leading to high false-positive rates. Mid-range thresholds (0.4–0.5) provide
a more balanced precision-recall trade-off, with overall accuracy peaking around 0.5.
Higher thresholds (0.6–0.9) improve precision for defaulters but drastically reduce recall,
meaning many actual defaults are missed. For credit risk applications where missing
defaulters is costly, a lower threshold (around 0.3–0.4) may be preferable, whereas higher
thresholds suit conservative acceptance strategies prioritizing fewer false positives.
LGBM:
• AUC-ROC Score for LGBM is 0.6579.
• ROC Curve:
18
The ROC curve for the LightGBM model shows the True Positive Rate (TPR) plotted
against the False Positive Rate (FPR) for different probability thresholds. The blue
curve consistently lies above the red diagonal baseline, indicating predictive performance
better than random guessing. Compared to the logistic regression ROC curve, this plot
appearsslightlymoreconvex, suggestingimproveddiscriminatorypowerindistinguishing
defaulters from non-defaulters. The AUC value (not shown here) would likely be higher
than that of the logistic regression model, reflecting LightGBM’s ability to capture more
complex, non-linear relationships in the data.
• F1 Score: 0.46361107043490996
• Precision: 0.6123791102514506
• Recall: 0.3729971724787936
• Confusion Matrix:
The confusion matrix for the LightGBM model shows that out of all non-defaulters
(0), the model correctly identified 4,740 (True Negatives) and misclassified 1,002 as
defaulters (False Positives). For defaulters (1), it correctly predicted 1,583 (True
Positives) but missed 2,661 cases (False Negatives).
19
• Precision-Recall Curve:
The Precision-Recall curve for the LightGBM model illustrates the trade-off be-
tween correctly identifying defaults (recall) and maintaining prediction accuracy
(precision) across different probability thresholds. Precision remains high at low
recall values, meaning the model is very accurate when predicting defaults but
initially captures only a small portion of actual defaulters. As recall increases,
capturing more true defaults, precision gradually declines, reflecting a rise in false
positives. Compared to the logistic regression curve you shared earlier, LightGBM
sustains higher precision for a broader range of recall values, indicating a stronger
balance between detecting defaults and limiting incorrect classifications.
• KS statistic: 0.2290 - indicates moderate discriminatory power, meaning the
LightGBM model can separate defaulters from non-defaulters with a maximum
distribution difference of 22.9%.
20
• Threshold tuning:
Table 2: LightGBM Performance Across Thresholds
Threshold Class Precision Recall F1-score Support Accuracy Macro F1
0.1 0 0.8571 0.0010 0.0021 5742
0.4255 0.2994
1 0.4252 0.9998 0.5966 4244
0.2 0 0.8016 0.0521 0.0978 5742
0.4475 0.3498
1 0.4338 0.9826 0.6019 4244
0.3 0 0.7356 0.2544 0.3781 5742
0.5187 0.4928
1 0.4649 0.8763 0.6075 4244
0.4 0 0.6859 0.5773 0.6270 5742
0.6049 0.6036
1 0.5290 0.6423 0.5802 4244
0.5 0 0.6405 0.8255 0.7213 5742
0.6332 0.5925
1 0.6124 0.3730 0.4636 4244
0.6 0 0.6098 0.9528 0.7436 5742
0.6223 0.5131
1 0.7327 0.1751 0.2826 4244
0.7 0 0.5973 0.9855 0.7438 5742
0.6097 0.4621
1 0.8379 0.1011 0.1804 4244
0.8 0 0.5958 0.9911 0.7442 5742
0.6083 0.4540
1 0.8825 0.0902 0.1637 4244
0.9 0 0.5803 0.9990 0.7342 5742
0.5840 0.3892
1 0.9412 0.0226 0.0442 4244
Threshold selection has a major impact on LightGBM’s classification performance. Very
low thresholds (0.1–0.3) yield extremely high recall for defaulters, but precision for non-
defaulters collapses, leading to large false-positive rates. A threshold around 0.4 offers the
most balanced performance, with accuracy at 60.5%, macro F1 at 0.6036, and relatively
evenprecision-recalltrade-offsforbothclasses. Higherthresholds(0.5–0.9)improvepreci-
sion for defaulters but sharply reduce recall, causing the model to miss a large proportion
of actual defaults. For credit risk purposes, where missing defaults is costlier than false
positives, a slightly lower threshold such as 0.4 is likely optimal.
8 Model explainability
Understanding why a model makes certain predictions is critical in credit risk modeling
to ensure transparency, regulatory compliance, and trust in decision-making.
• coef fromLogisticRegression–Providestheestimatedcoefficientsforeachfeature,
showing the direction (positive or negative) and magnitude of their impact on the
21
predicted probability of default.
• Feature Importance (Tree Models) – Ranks features based on their contribution
to reducing prediction error in decision tree–based algorithms such as LightGBM
or XGBoost.
• SHAP Values – Quantify each feature’s contribution to individual predictions
(local) and overall model behavior (global), offering an interpretable and consistent
framework for understanding model outputs.
• Partial Dependence Plots (PDPs) – Illustrate the marginal effect of a selected
feature on the predicted outcome, helping visualize non-linear relationships and
threshold effects.
Logistic regression:
The plot above presents the feature coefficients for the logistic regression model. Features
such as Unemployed, debt to income, and economic cycle have the highest positive coef-
ficients, indicating a strong association with increased default risk. Conversely, features
like collateral, credit score, and employment status (especially Self-Employed) exhibit
negative coefficients, suggesting they contribute to a lower likelihood of default. These
results reflect expected credit risk patterns and support the model’s interpretability.
22
LGBM:
TheplotabovedisplaysfeatureimportancesfromtheLightGBMmodel. Credit scoreand
credit history length are the most influential variables, followed closely by interest rate
and debt to income, indicating that borrower creditworthiness and loan affordability are
key predictors of default. Additional important features include various income-related
ratios and loan characteristics. Features such as marital status, unemployment, and
specific loan purposes contribute less to the model’s decisions, suggesting their relatively
limited predictive value.
The SHAP force plot above explains how each feature contributed to this specific
prediction. The base value (average model output) was around −0.31, and the final
prediction shifted to −1.10, indicating a stronger prediction toward non-default.
Red features on the left, like economic cycle and debt to income, pushed the prediction
toward default. However, several strong blue features pulled it in the opposite direction,
includingloan to income ratio,installment to income,credit history length,credit score,
and even Unemployed (which in this specific case had a negative impact). Overall, the
combination of these features led the model to predict a low default probability for this
instance.
23
The SHAP summary plot illustrates the overall influence of each feature on Light-
GBM’s predictions. Debt-to-income, Unemployed, and economic cycle have the strongest
impact on default predictions. Red dots (high feature values) on the right indicate a push
toward higher default risk, while blue dots (low values) on the left suggest lower risk.
For example, high debt-to-income ratios and being unemployed push predictions toward
default, while high credit scores and presence of collateral reduce the risk. This plot high-
lights both the importance and directional effect of each feature across all observations.
24
These plots demonstrate how individual features influence the LGBM model’s predic-
tion of loan default probability, assuming all other features are held constant:
• Credit Score: There is a clear negative relationship—higher credit scores reduce
the default probability, as expected in credit risk modeling.
• Debt-to-Income Ratio: A strong upward trend is visible—higher DTI ratios
significantly increase the likelihood of default, highlighting borrower overextension.
• Loan Amount: The default risk initially rises slightly with loan amount and then
levels off, suggesting LGBM learns some nonlinearity but de-emphasizes loan size
beyond a point.
• Interest Rate: A slight positive trend exists—higher interest rates correlate with
increased risk, though the effect is not dramatic.
• Expenses: Higher expenses increase the default probability, especially at lower
expense levels, where a sharp jump is visible.
• Income: There’s a steep drop in default risk at very low income levels, but beyond
a certain income threshold, additional income doesn’t significantly impact the risk.
LGBM captures nonlinear risk patterns effectively. Creditworthiness (credit score),
financial stress (DTI, expenses), and affordability (income) are key drivers of predicted
defaults.
25
What are the strongest predictors of default?
The analysis shows that the strongest predictors of default are credit score, debt-to-
income ratio, credit history length, interest rate, economic cycle, and employment status.
Lower credit scores and shorter credit histories are strongly associated with higher default
risk, while high debt-to-income ratios indicate borrower overextension and significantly
increase the likelihood of default. Higher interest rates also correlate with elevated risk,
reflecting risk-based pricing effects. Macroeconomic conditions captured by the economic
cycle play a role, with downturns increasing default probability. Additionally, being
unemployed is a critical risk factor, as it directly affects a borrower’s repayment capacity.
Together, these variables capture key aspects of creditworthiness, financial stress, and
loan affordability, making them the most influential drivers of predicted default across
both Logistic Regression and LightGBM models.
Are there nonlinear effects or thresholds?
The analysis of partial dependence plots from the LightGBM model reveals several non-
linear effects and threshold behaviors in default risk. Credit score demonstrates a clear
threshold pattern, with default probability declining sharply beyond a certain score and
stabilizing at higher values. Debt-to-income ratio shows a steep, nonlinear increase in risk
once it surpasses a critical level, indicating a tipping point where borrower overextension
significantlyraisesthelikelihoodofdefault. Loanamountdisplaysamildnonlineartrend,
with risk increasing initially and then plateauing, suggesting limited incremental effect
beyond a certain size. Both income and expenses exhibit threshold effects as well: very
low incomes and low-to-moderate expenses have a strong impact on default probability,
but the effect levels off at higher values. These findings confirm that default risk is influ-
enced by nonlinear relationships, with specific feature ranges exerting disproportionate
influence on model predictions.
9 PD calibration
In probability of default (PD) modeling, the raw probability outputs from classifiers
often do not perfectly align with the true observed default rates, leading to over- or un-
derestimation of risk. Calibration techniques are applied to adjust these outputs so that
predicted probabilities better reflect actual outcomes. Platt Scaling, a logistic regres-
sion–based method, fits a sigmoid function to the model’s scores, providing a parametric
adjustment suitable for well-behaved probability distributions. Isotonic Regression, a
non-parametric alternative, fits a monotonic function to the scores, allowing greater flex-
ibility in capturing irregular calibration patterns.
Model calibration is evaluated using visual diagnostics such as calibration curves, which
26
plot predicted probabilities against observed default frequencies, and binned PD vs. ac-
tual default rate plots, where predictions are grouped into deciles or buckets for com-
parison. These diagnostics help assess whether the model’s probability estimates are
reliable and whether specific ranges require adjustment. A well-calibrated PD model en-
sures that predicted default probabilities are both accurate and actionable for credit risk
decision-making.
The calibration curve for the logistic regression model shows the relationship between
predicted default probabilities and the actual observed default rates. The solid blue line
(calibrated model) closely follows the dashed orange diagonal, which represents perfect
calibration,indicatingthatthemodel’sprobabilityestimatesaregenerallyaccurateacross
the entire range. Minor deviations are visible at lower and higher probability ranges,
where the model slightly underestimates risk for low scores and slightly overestimates for
high scores. Overall, the alignment suggests that after calibration, the logistic regression
model produces well-calibrated probability of default estimates, making them reliable for
credit risk decision-making.
27
The calibration curve for the LightGBM model using isotonic regression shows that
predicted default probabilities align very closely with actual observed default rates across
most probability ranges. The blue calibration line almost perfectly follows the orange
diagonalofperfectcalibration, indicatingthatisotonicregressionhaseffectivelycorrected
any systematic bias in the model’s raw probability outputs. Minor deviations occur
at the very low and mid-high probability ranges, but these are minimal and unlikely
to impact decision-making. Overall, the calibrated LightGBM model produces highly
reliableprobabilityof default estimates, making itwell-suitedforoperational useincredit
risk assessment.
The binned Probability of Default (PD) vs. actual default rate plot for the LightGBM
model shows an almost perfect alignment between predicted and observed defaults across
28
all bins. The blue points closely follow the orange “perfect calibration” line, indicating
that the model’s predicted PDs are highly accurate and unbiased after calibration. This
suggests that LightGBM not only ranks borrowers well in terms of risk but also produces
probability estimates that can be used directly for risk-based decision-making, such as
setting interest rates or determining credit limits.
10 Risk-Based Strategy Simulation
This section outlines how model-generated probability of default (PD) estimates can
be operationalized into a risk-based lending strategy. By segmenting borrowers into
predefined risk bands, the bank can tailor approval, pricing, and monitoring policies to
balance portfolio growth with credit risk control. Simulations of these strategies provide
a data-driven view of potential loss exposure, profitability, and the trade-offs between
risk and return.
Segment into risk bands:
df[’Risk_Band’] = pd.qcut(df[’PD’], q=[0, 0.05, 0.15, 1],
labels=[’Low’, ’Medium’, ’High’])
The PD values were segmented into three risk categories using quantile-based binning:
Low risk (bottom 5% of PDs), Medium risk (next 10%), and High risk (remaining 85%).
This classification enables the bank to target lending and pricing strategies more effec-
tively based on predicted default risk.
Simulate business strategies:
LGD = 0.5
EAD = df[’loan_amount’]
df[’Expected_Loss’] = df[’PD’] * LGD * EAD
Thissimulationappliesarisk-basedlendingpolicywherehigh-riskapplicantsarerejected,
medium-riskapplicantsareacceptedwithaddedmargins,andlow-riskapplicantsarefully
accepted. The expected loss for each borrower is calculated using the formula:
Expected Loss = PD×LGD×EAD
where LGD (Loss Given Default) is assumed to be 50%, EAD (Exposure at Default)
istheloanamount, andPD isthemodel-predictedprobabilityofdefault. Thisframework
enables the bank to estimate potential financial losses under different risk strategies.
29
PD vs Actual Default Rate per Decile:
The plot compares the average predicted probability of default (PD) with the actual de-
fault rate across deciles of predicted risk. The close alignment of the two curves indicates
that the model’s probability estimates are well-calibrated, as higher predicted PDs con-
sistently correspond to higher observed default rates. This strong monotonic relationship
confirms the model’s ability to effectively rank borrowers by credit risk.
Applicant Volume per Risk Band:
The distribution of applicants across risk bands reveals a heavily skewed profile, with
the majority falling into the high-risk category, followed by a smaller proportion in the
medium-risk band and a minimal share in the low-risk group. This imbalance indicates
that most applicants have a relatively high predicted probability of default, which has
30
important implications for credit approval rates, portfolio risk, and the potential prof-
itability of lending strategies.
Expected loss vs decision threshold:
The relationship between expected loss and the PD threshold shows a steep increase
in total expected losses as the acceptance threshold moves beyond approximately 0.3,
reflecting the growing inclusion of higher-risk borrowers. The curve flattens at higher
thresholds, indicating that most potential losses are concentrated among applicants with
moderate to high predicted default probabilities. This pattern highlights the trade-off
between portfolio growth and credit risk when adjusting approval cut-offs.
11 Recommendation
This section outlines actionable recommendations derived from the Probability of De-
fault (PD) model analysis, addressing credit policy adjustments, expected loss impact,
integration into decision workflows, and model stability across customer segments.
What credit policies should be updated?
The bank should consider introducing differentiated lending terms based on the predicted
PD risk bands: accepting low-risk applicants with competitive rates, applying higher
interestmarginsorcollateralrequirementsformedium-riskapplicants, andrejectinghigh-
risk applications to limit exposure.
What is the impact of the model on expected losses?
The model’s integration into the decision-making workflow can significantly reduce ex-
pected losses by enabling risk-based pricing and early rejection of high-risk cases, as
demonstrated in the expected loss simulations.
31
How can the model be integrated into loan decision workflows?
The PD model should be embedded into the loan origination system to provide real-time
risk scoring during application processing, allowing for dynamic and consistent credit
decisions.
Is the PD model stable across different customer segments?
Continuous monitoring is essential to ensure model stability across different customer
segments. Periodic recalibration should be conducted to account for shifts in borrower
profiles, market conditions, or macroeconomic factors.
32